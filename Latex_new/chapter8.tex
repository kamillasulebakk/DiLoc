% !TEX root = main.tex
\documentclass[a4paper, UKenglish, 11pt]{uiomaster}
\usepackage{lipsum}
\usepackage[subpreambles=true]{standalone}
\usepackage[table,xcdraw]{xcolor}
\usepackage{hyperref}
\usepackage{xcolor}

\begin{document}
\chapter{Discussion and Final Remarks} \label{chap:discussion}
In this chapter, we will delve into the key findings derived from the fully connected neural network (FCNN) and the convolutional neural network (CNN) concerning the localization and identification of current dipole moments. In Section \ref{sec:results}, we will present an overview of the main results of this thesis, emphasizing the most noteworthy and pivotal findings. Subsequently, sections \ref{sec:architecture} and \ref{sec:data_set} will delve into the architectural aspects of the networks and the data set employed for training and result acquisition before discussing computing time and head-specific modeling in Section \ref{sec:time}. Section \ref{sec:convdip} is dedicated to contextualizing our results within existing research. Finally, Section \ref{sec:outlook} and Section \ref{sec:Conclusion} provide some final remarks on the work done in this thesis, along with a discussion of future prospects following our approaches.

\section{Summary of Main Results} \label{sec:results}
Throughout the various approaches to the EEG inverse problem in this thesis, we have systematically evaluated the neural networks' performance using MED, MSE, and MAE. In Table \ref{table:sum_main_results}, we present a summary of the main results throughout this thesis.

\begin{table}[]
\hspace*{-4cm} % Adjust the value as needed to move the figures left
\begin{tabular}{c|cccccc|}
\cline{2-7}
\multicolumn{1}{l|}{}                                                                                               & \multicolumn{6}{c|}{\cellcolor[HTML]{CBCEFB}\textbf{Summary of Main Results}}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \\ \cline{2-7}
\multicolumn{1}{l|}{}                                                                                               & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}Singe Dipole:\\ FCNN\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}Single Dipoles:\\ CNN\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}Two Dipoles:\\ FCNN\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}Two Dipoles:\\ CNN\end{tabular}} & \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}Dipole with \\ Magnitude:\\ FCNN\end{tabular}} & \cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}Dipole \\ Population:\\ FCNN\end{tabular} \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}$d$}                                                                   & \multicolumn{1}{c|}{3}                                                                                    & \multicolumn{1}{c|}{3}                                                                                     & \multicolumn{1}{c|}{6}                                                                                   & \multicolumn{1}{c|}{6}                                                                                  & \multicolumn{1}{c|}{4}                                                                                                & 5                                                                                            \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}\begin{tabular}[c]{@{}c@{}}Coordinate with\\ highest MAE\end{tabular}} & \multicolumn{1}{c|}{z}                                                                                    & \multicolumn{1}{c|}{z}                                                                                     & \multicolumn{1}{c|}{z}                                                                                   & \multicolumn{1}{c|}{z}                                                                                  & \multicolumn{1}{c|}{y}                                                                                                & z                                                                                            \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}MED($x, y, z$) {[}mm{]}}                                               & \multicolumn{1}{c|}{1.3}                                                                                  & \multicolumn{1}{c|}{1.8}                                                                                   & \multicolumn{1}{c|}{8.71}                                                                                & \multicolumn{1}{c|}{11.78}                                                                              & \multicolumn{1}{c|}{2.815}                                                                                            & 7.85                                                                                         \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}MSE($x, y, z$) {[}mm$^2${]}}                                           & \multicolumn{1}{c|}{0.782}                                                                                & \multicolumn{1}{c|}{1.643}                                                                                 & \multicolumn{1}{c|}{38.978}                                                                              & \multicolumn{1}{c|}{69.306}                                                                             & \multicolumn{1}{c|}{3.720}                                                                                            & 50.359                                                                                       \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}MAE($x, y, z$) {[}mm{]}}                                               & \multicolumn{1}{c|}{0.662}                                                                                & \multicolumn{1}{c|}{0.898}                                                                                 & \multicolumn{1}{c|}{4.340}                                                                               & \multicolumn{1}{c|}{5.731}                                                                              & \multicolumn{1}{c|}{1.405}                                                                                            & 3.961                                                                                        \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}MAE$_A$ {[}nAm{]}}                                                 & \multicolumn{1}{c|}{-}                                                                                    & \multicolumn{1}{c|}{-}                                                                                     & \multicolumn{1}{c|}{-}                                                                                   & \multicolumn{1}{c|}{-}                                                                                  & \multicolumn{1}{c|}{0.539}                                                                                            & 0.33                                                                                         \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}MAE$_R$ {[}mm{]}}                                                      & \multicolumn{1}{c|}{-}                                                                                    & \multicolumn{1}{c|}{-}                                                                                     & \multicolumn{1}{c|}{-}                                                                                   & \multicolumn{1}{c|}{-}                                                                                  & \multicolumn{1}{c|}{-}                                                                                                & 0.76                                                                                         \\ \hline
\end{tabular}
\caption{Summary of main results of the thesis.}
\label{table:sum_main_results}
\end{table}


\subsection{Predicting Dipole Position}

% https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3683142/
% \rednote{It may be possible to achieve a desirable (≪1 cm) level of accuracy in many new and existing EEG-based functional cortical imaging studies by: (1) Using a model that accurately represents head geometry and electrode placement, (2) estimating skull conductance from the data (Huang et al. 2007; Lew et al. 2009), and (3) finding equivalent dipole source locations accounting for synchronous activity within relatively small, compact cortical patches (Akalin Acar et al. 2009)—such as those derived by ICA decomposition of high-density data (Delorme et al. 2012). In particular, this level of accuracy should allow useful comparison of data source locations across EEG and blood oxygenation level-dependent (BOLD) studies.}

As highlighted in Chapter \ref{chap:simple_dipole_FCNN}, it is possible to achieve a desirable level of accuracy, specifically an error margin smaller than 10 mm, when addressing the inverse EEG problem. This achievement is consistently observed when utilizing head models that accurately represent head geometry and electrode placement, as exemplified by the New York Head Model applied through the LFPy 2.0 software.

In all our fully-connected feed-forward neural network (FCNN) approaches, we have consistently succeeded in localizing current dipole moments with precision that remains under this 10 mm threshold. Whether the task is to predict single dipole locations, two dipole locations at once, or dipole locations alongside dipole characteristics, this level of precision is consistently maintained. Similarly, the convolutional neural network (CNN) effectively localizes single current dipoles, albeit exhibiting predictions just above this threshold when tasked with concurrent localization of two current dipoles.

For single dipole localization, the FCNN achieved an error of less than 10 mm for 99.995$\%$ of the test samples, while the CNN reached the same level of accuracy for 99.815$\%$ of the samples. Subsequently, when the FCNN was challenged to predict varying magnitudes in addition to dipole location, it maintained an accuracy below 10 mm for 99.505$\%$ of the test data. However, when faced with the task of predicting the center of dipole populations of varying sizes, only 79.680$\%$ of the test sample predictions fell below the desired threshold. In the scenario where the FCNN and CNN were tasked with predicting the positions of two current dipoles contributing to a collective EEG signal, only 73.055$\%$ and 50.460$\%$ of the predictions, for the FCNN and CNN respectively, exhibited an accuracy smaller than 10 mm.

The accuracy, quantified by the Mean Euclidean Distance (MED), exhibited a diminishing trend as the output dimension 'd' of the FCNN increased. A similar trend was also observed in the CNN, with the MED notably decreasing when predicting the coordinates of a single dipole compared to predicting the spatial coordinates of two dipoles simultaneously.


\subsubsection{CNN in Comparison to FCNN}
When comparing the MED between the two networks, we observed that the FCNN yields more satisfactory results compared to the CNN. This preference for the FCNN's performance was the basis for our decision to proceed with the FCNN architecture when extending the EEG inverse problem to encompass the identification of dipole strength, as well as population strength and radius, for an area of active correlated current dipoles.

With this said, we emphasize that the architecture and hyperparameters of the FCNN were explored in greater detail compared to what was done for the CNN. While the FCNN provided the most satisfactory results in our work, we must acknowledge that a CNN might perform equally well or even better with a more optimized architecture and hyperparameters. Consequently, we cannot definitively conclude whether or not the performance of a neural network in predicting spatial coordinates is positively influenced by the preservation of spatial structure in the input data, as is the case for CNNs in general. Given more time, it would have been valuable to further investigate this possibility.

\subsubsection{Error Variation}
\rednote{Include rmse in table? remove mse?}
Following the definition of MAE and RMSE, it can be shown that RMSE is always greater than or equal to MAE. Using these metrics together, offer valuable insights into the variation in error measurements within a data set: the greater difference between them, the greater the variance in the individual errors in the sample. If the RMSE is equal to the MAE, then all the errors are of the same magnitude, which is considered ideal.

With respect to the MSD between predicted and target spatial coordinates, we observes that the RMSE was consistently somewhat higher than the measured MAE for all positional predictions across all network approaches. This discrepancy suggests the presence of relatively larger errors in the data set, meaning that the predictions of the networks and do not consistently align with the overall distribution of errors. This disparity between MAE and RMSE is most pronounced when localizing two dipoles simultaneously and when predicting the centers of dipole populations with varying radii and strengths. While this divergence indicates room for improvement in the models, it is important to acknowledge that some variation in neural network predictions is expected.


%Following the definintion of RMSE and MAE, it can be shown from the triangle inequality, the RMSE is always greater than the MAE. Using these to metrices together can help in assess the variation in the error measurements within a data set: the greater difference between them, the greater the variance in the individual errors in the sample. If the RMSE is equal to the MAE, then all the errors are of the same magnitude.

% With respect to the MSD between predicted and target spatial coordinates, we observes that the RMSE consequently was somewhat higher than the measured MAE for all postional predictions for every network approach. This suggests that there might be some relatively larger errors in the data set, as RMSE gives more weight to larger errors due to the squaring of the errors. This gives an indicator of that the errors are somewhat dispersed, and there are likely a few data points with relatively large errors that contribute to this higher RMSE. This discreapancy between MAE and RMSE is most apparent for when localizing two dipoles at once, as well as when predicting the center of dipole populations with varying radii and stregths. This is something than can be improved, however some variations in predictions of neural networks are to be expect.

%  for the FCNN, we observed notably high values when instructing the network to simultaneously localize two current dipoles and predict population center, radius, and strength. These specific values are summarized in Table \ref{table:sum_main_results}. A high MSE often implies that, on average, the prediction errors exhibit a substantial variance and do not consistently align with the overall distribution of errors. In simpler terms, there is a wide spread in the accuracy of predictions across different data points, which may suggest that the model's performance is inconsistent. Initially, this implies that the model generates predictions with relatively large errors, deviating significantly from the true values.
%
% All errors in the above example are in the range of 0 to 2 except 1, which is 5. As we square it, the difference between this and other squares increases. And this single high value leads to higher mean. So MSE is influenced by large deviators or outliers.
%
% However, to determine whether these notable "high" MSE values are of concern or not, we consider a specific example. The mean absolute errors for the coordinates in the problem involving the prediction of the center, radius, and strength of a current dipole population were measured at 3.819 mm, 4.342 mm, and 3.722 mm for the $x$-, $y$-, and $z$-coordinates, respectively. If we were to treat these as the actual absolute errors for a prediction (and not the mean of the whole data set), the corresponding squared error would be 47.29 $mm^2$, and the Euclidean distance would equal 6.87 mm.
%

% Suppose we had $n$ predictions, all with this exact same squared error value, the total MSE for the predictions would be the same value -- 47.29 $mm^2$. This demonstrates that a model can yield satisfactory results in terms of the Mean Euclidean Distance (MED) of its predictions, with minor deviations in each spatial coordinate, yet still produce an initially high MSE. In other words, relatively small variations in coordinate space, as in our case, can lead to relatively high MSE. This indicates that these "high" values do not necessarily suggest that the model delivers "poor" predictions.

\subsubsection{Predicting Spatial Coordinates}
Predicting the $z$-direction of the dipoles yielded varied results across the different approaches to the inverse problem. In most of the neural network models, a slightly higher MAE was observed in the $z$-coordinate. An initial explanation for this outcome might be rooted in the construction of the EEG method, where recording electrodes are situated along both the right-left ($x$-axis) and front-back ($y$-axis) directions, whereas electrodes along the $z$-axis are primarily placed on top of the head.

An exception to the elevated MAE in the $z$-coordinate emerged when predicting dipoles with varying strengths, as the $y$-coordinate exhibited the highest MAE among the spatial coordinates. One might initially attribute the reason why the prediction of the $z$-coordinate did not exhibit the highest MAE in that specific scenario to the fact that, in most of the corresponding data, dipoles in this particular case exhibited significantly larger strengths (with randomly assigned strengths ranging from 1 to 10 nAm) as opposed to when predicting dipoles with a constant strength of 1 nAm, making all spatial coordinates of the dipoles easier to detect, even within the constraints of electrode placement and skull conductivity. However, this explanation is challenged when considering scenarios involving varying strengths for dipole populations. In such cases, the $z$-coordinate once again demonstrated the highest MAE, despite the increased strength of the dipoles.

Another observation that contradicts the notion that $z$-coordinates of the dipoles are inherently harder to detect accurately is the alternation in coordinates displaying the highest MSE. While, in terms of MSE, the $x$-coordinate consistently displayed the lowest value, the predictions for the $y$- and $z$-coordinates alternated as having the highest MSE. Thus, it is reasonable to conclude that the somewhat poorer predictions in the $z$-coordinate, as observed in some models, may be influenced by various factors, potentially including variability in the measurement process, rather than being solely attributed to the spatial properties of the $z$-coordinate of the dipole's location itself.



\subsubsection{Predicting Dipole Strength and Population Radius}

When predicting dipole strength, we utilized two alternative versions of the FCNN, as presented in Chapter \ref{chap:extended_FCNN}. In the first version of the network we focused on single dipoles, where the strength exhibited variation across the data set. The primary objective of this network was to predict the spatial coordinates for each current dipole, alongside determining the magnitude of the current potential. Within this configuration, our model gave fulfilling spatial predictions, yielding a MED of 2.815 mm. The magnitude predictions exhibited an associated MAE$_{A}$ of 0.539 nAm. This error corresponded to a relative error of 6.00$\%$ within the simulated range of 1-10 nAm.

In contrast, our second, more complex version of the alternative FCNN was tailored to address populations of active correlated current dipoles characterized by varying strengths and radii. In this scenario, we achieved a MED of 7.85 mm, and a MAE$_A$ in magnitude measuring 0.33 nAm. Remarkably, the transition from the previous model to this more complex version led to a 5 mm increase in MED, while the MAE$_{A}$ decreased to 0.33 nAm, which corresponds to 3.6$\%$ of the target range.

These differences in model performance can be attributed to the inherent correlation between the prediction of radius and strength. The manner in which our simulated data is generated establishes a strong interdependence between these two variables. As a consequence the model's optimization process might indirectly give more importance to reducing errors in radius and strength, even though we in our customized cost function, gained equal weighting to the target coordinates and the absolute error associated with the predicted radius and strength. In other words, when we reduce the error in radius prediction, it also reduces the error in strength prediction because they are strongly related. In contrast, the position of the dipole, as reflected in the Euclidean distance, is not correlated with strength or radius. Therefore, while reducing errors in strength or radius can indirectly benefit each other due to their correlation, this does not translate into a reduction in the error of the Euclidean distance.

In practice, this results in varying degrees of emphasis on Euclidean distance and absolute errors in strength and radius due to their strong correlation -- which for the most complex version of the alternative FCNN, lead to an significantly improvement in MAE$_{A}$ for the dipole strength. To achieve equal importance for all target values, we could have considered addressing these correlations in the cost function design by assigning weighting factors to determined the relative importance of each of the terms contributing to the total cost. This would most likely opened up for an even more precise accuracy in the center of the current dipole populations.

The MAE$_{r}$ of the network measured 0.74 mm for the predictions of the radius on the test samples, corresponding to an relative error of 5.4$\%$ of the target range. With this level of accuracy, we can be confident that the network effectively distinguishes between very small, medium, and large populations of active correlated current dipoles. This confidence is supported by the findings in Chapter \ref{chap:extended_FCNN}, as shown in Figure \ref{fig:area_errors}, where it was observed that the accuracy of the predictions in position remained consistent across varying dipole population sizes. This consistency reaffirms the network's ability to handle a wide range of dipole population sizes, making it a robust tool for the task.

% The explaination to why the magnitude target loss and radius target loss tend to have the same shape on the loss curve, is that the magnitude of the signal is directly proportional to the radius of the population (as each dipole within a population hols the same magnitude, meaning that the larger the population, the larger the magnitude of the total signal). We do therefore have a corelation beetwen these two variables, and we can think of the setup like if we were to weightened the magnitude and radius twice as much as the target coordinates.
% Compared to the problem where the network were to only predict magnitude and not radius, we do see from the result section that when preedicting both magnitude and radius, the training takes a shorter time to fininsh, and moreover we get more precise predictions.

\subsubsection{Predicting Multiple Dipoles}
When instructing the neural networks to simultaneously output the positions of two current dipoles, we observed the highest positional errors, measured as the Mean Euclidean Distance, across all scenarios. However, the FCNN provided accurate predictions below the 10 mm threshold, whilst the CNN gave predictions just above this threshold.
%With two dipoles, the neural network must account for interactions and signal interferences between them.
The increased positional error when predicting an additional dipole position can be attributed to various factors. Firstly, predicting dipoles in close proximity presents a greater challenge for neural networks. The increased challenge arises from the strong electromagnetic field interactions that occur when dipoles are located near each other.  These interactions give rise to complex patterns, thereby necessitating the neural network to learn more intricate relationships.

To address these difficulties, one potential approach would be to manipulate the available dipole positioning, ensuring that dipoles are always separated by a minimum or constant distance. However, this approach might limit the network's ability to handle real-world scenarios, and its impact on prediction performance remains uncertain. Alternatively, employing a more complex neural network architecture with a greater number of adjustable parameters to accommodate the data patterns is an option. Nevertheless, this also increases the risk of overfitting.

Despite the increased positional error compared to scenarios requiring fewer dimensions in the network's outputs, we found the results satisfying, given that the MED for the dipoles in the FCNN predictions remained below the 10 mm threshold.


\section{Network Architecture} \label{sec:architecture}
The selection of hyperparameters for the neural networks underwent an iterative process of trial and error, leading to the architecture used in the presented results chapters. A notable feature of the FCNNs is the deliberate use of various activation functions within the hidden layers. Following the initial linear transformation of input data, ReLU activation was employed, followed by the hyperbolic tangent in subsequent hidden layers. For the extended FCNNs predicting outputs with multiple units, we opted for the sigmoid activation function to deal with normalized target values, instead of the standard linear layer often used in regression problems.

The introduction of diverse activation functions across the network layers allows for varying degrees of nonlinearity \rednote{mappings}. In the best case scenario, it can enhance the network's ability to generalize effectively to unseen data by capturing distinct patterns, offering a more comprehensive representation of the data. However, it is important to note that this diversity may have made the networks more complex than necessary. While increased flexibility can be advantageous for certain problems, it also raises the risk of overfitting.

Our primary objective in developing various neural network approaches was not to attain perfection, but rather to investigate the network's adaptability in addressing the EEG inverse problem across diverse levels of complexity. This complexity included variations in output strengths and radii of neural activity that generated the input EEG signal. In most of our approaches, we successfully achieved localization errors smaller than 10 mm.

However, in clinical contexts, there exists a considerable distinction between a localization error of 10 mm and 1.0 mm. It is important to underscore that an error of 1.3 mm, as observed in the case of the FCNN predicting dipole position alone, is significantly more accurate than the 8.71 mm error observed when predicting the position of two current dipoles. Both from a modeling perspective and in clinical practice, smaller errors are preferable. Nevertheless, it is essential to recognize that further diminishing the error beyond a certain point may lose clinical significance. While there may be a noticeable percentage improvement in the model's loss, within the context of cortical measurements, such improvements may not be practically discernible. In other words, striving for additional model optimization can be beneficial up to a certain point, but beyond that, it may not necessarily result in substantial clinical advantages.

% In summary, we have observed that a "simpler" neural network can predict the locations of modeled neural activity dipoles with a localization error smaller than 10 mm. There is room for further model optimization, but it is essential to acknowledge that, at some point, the pursuit of further optimization may offer limited advantages in clinical procedures.

% - [ ] Initialization

\section{Data Set} \label{sec:data_set}
In our neural network approach for localizing current dipoles, we employed the New York Head Model integrated into the LFPy Python Module to simulate EEG data. To enhance the realism of the data, we introduced normally distributed noise, characterized by a mean of 0 and a standard deviation amounting to 10$\%$ of the standard deviation observed in the simulated EEG recordings, into the final data set used for both training and validation. In subsequent testing on noise-free data, we found that, in most cases, the neural networks produced predictions with positions falling below 10 mm.

One notable advantage of working with self-simulated data is the high degree of control it affords. This control allowed us to create data for specific scenarios, including the cases with two dipoles, dipoles featuring varying strengths, and populations of dipoles exhibiting varying radii.  Having access to the correct answers for training and evaluation enabled us to effectively assess the performance of the neural networks. Furthermore, the New York Head Model, offering 74,382 discrete points for dipole source localization, facilitated the creation of an extensive data set suited for both training and testing.

Nevertheless, it is essential to acknowledge that our networks have not been tested on real-world EEG recordings. Consequently, it is highly probable that the extent to which our networks faithfully capture the complexity, noise, and variability present in clinical EEG data will not align with the performance demonstrated on simulated validation and test data. This disparity is a well-documented limitation in the field of machine learning, where the performance achieved with both simulated and experimental data may not consistently translate directly to specific real-world scenarios. Consequently, it is expected that our framework's performance will be suboptimal when directly applied to EEG recordings from actual patients.

In future studies, there are valuable insights to be gained by exposing the networks to actual patient recordings, which would provide an opportunity to assess their performance in genuine clinical contexts. Nonetheless, as just outlined it is highly likely that our network approaches may not perform optimally with real experimental data. In the event that these approaches is applied within an authentic clinical setting, such as for a patients requiring brain surgery, it may be plausible to construct patient-specific head models that can be integrated into our framework, subsequently training the networks with data extracted from the patients head models. Nonetheless, it is crucial to underscore that the effectiveness of such an approach would necessitate empirical testing within a real-world healthcare environment, and the degree of success in practice remains uncertain.

\section{Computation Speed} \label{sec:time}
The time required for training the neural networks ranged from approximately 1.5 to 9.5 hours, depending on the number of training epochs and network complexity. All training processes were conducted on a personal laptop without the utilization of GPU acceleration. It is worth noting that running these training processes on high-performance GPUs or employing parallelization techniques holds the potential to significantly enhance the efficiency of the training procedure, which could be an area of interest for future studies. Nonetheless, for the objectives of our current work, the training times were found to be satisfactory and aligned with our research goals.

Training neural networks for challenging tasks inevitably demands a significant amount of computational resources. For patient-specific head model construction, followed by training the networks using data extracted from these individualized models, one can anticipate a substantial requirement for processing power and time. The computational speed necessary for training is inherently influenced by the number of parameters $\boldsymbol{\theta}$ to be adjusted, as well as the complexity of the problem and its solution space. In the context of solving the EEG inverse problem, reducing the spatial resolution of the neural positions of interest could potentially accelerate training. However, such a reduction would likely lead to a decrease in model accuracy.

Once a network is fully trained, it generally exhibits efficient execution times for individual samples. It is worth noting that we did not precisely measure the time taken for our network to predict a single sample. Nevertheless, the rapid execution observed for a batch of 20,000 samples suggests that the time required for processing a single sample would be considerably shorter. An important point to note is that even though head-specific models would require time to develop, once fully trained, they are likely to require very short time for outputting predictions -- potentially making neural networks invaluable tools in clinical applications of EEG.

\section{Contextualizing: Existing Research on \\ ConvDip} \label{sec:convdip}
\rednote{Depth!}
% The trained network needs <40 ms for a single prediction. Our results qualify ConvDip as an efficient and easy-to-apply novel method for source localization in EEG data, with high relevance for clinical applications, e.g., in epileptology and real-time applications.

Numerous studies have explored the use of neural networks to address the EEG inverse problem. Among these, ConvDip, a convolutional neural network, stands out as a classification-based solution, that aims to predict the location of multiple dipole clusters along with the extent of these populations \cite{hecker2021convdip}.

ConvDip tackled the EEG inverse problem using simulated EEG data, from dipole populations of different extent. The network demonstrated an ability to produce inverse solutions from single time points of EEG data and provided plausible results for real EEG recordings from human participants. The network was trained on samples containing 1 to 5 source clusters and proved effective in handling even more complex cases. It strived to predict the correct source size and localize sources at varying depths, a task achieved by categorizing source extents into five sizes.

To generate realistic training data, the researchers constructed a source model with 5,124 dipoles distributed across the cortex, simulating 31 recording electrodes and incorporating real noise from EEG recordings. Additionally, they employed an alternative head model for test data to prevent overoptimistic results. The training data set contained 100,000 samples, while the test data set comprised 1,000 samples.

In ConvDip's case, EEG data was interpolated into a 2D image grid preserving spatial information and without the introduction of new. The network's output vector contained 5,124 elements, corresponding to dipole positions in the source model. Performance was evaluated by comparing the network's active output nodes with the true distribution of dipoles, with node activation indicating the network's prediction of a dipole source at a specific location. The network's ability to estimate source extent was assessed by evaluating the proximity of neighboring outputted dipoles.

ConvDip demonstrated robust localization accuracy across different source depths. However, it exhibited reduced accuracy in estimating source extent with increased depth. This specific relationship was not explored in our work. For single dipole populations, ConvDip achieved a mean localization error of 11.05 mm, somewhat higher than our FCNN's error of 7.85 mm when prediction the center of our constructed spherical populations. Notably, direct comparisons between these networks are intricate due to variations in head models and data set characteristics. Our training data featured more positions for localizing dipoles, allowing our FCNN to capture finer details for the specific head model. Furthermore, our evaluation data set lacked noise, making ConvDip's performance on more complex, noisy data noteworthy.

ConvDip was tested on real EEG data from human subjects. While the results were promising, it most of all emphasized the importance of individual variability in head anatomy, suggesting individual neural networks to be trained based on specific MRI data for each subject. This approach, though effective, raised concerns regarding computing time. To address this, an individual transfer learning method was suggested, where a network trained on one subject's anatomy could be fine-tuned for subsequent subjects with new training data. This transfer learning method holds potential relevance for our own approach.

ConvDip represents one of the numerous neural network-based approaches for addressing the EEG inverse problem. Although our approach diverges from ConvDip in terms of architectural design, several overlapping research aspects have been explored.

In this work, we have focused on training the neural network for localizing no more than two dipoles simultaneously. Nevertheless, it is worth noting that the code framework allows for simulating and training the neural network to handle an arbitrary number of dipoles, which could be a subject of interest for future studies.

\section{Conclusion} \label{sec:Conclusion}
This thesis is centered on addressing the EEG inverse problem, which involves the localization of neural populations responsible for specific EEG signals. To analyze EEG measurements, neural sources are often approximated as current dipoles. The New York Head Model, accessible through the LFPy 2.0 Python module, was utilized to simulate realistic EEG data. This data served as the foundation for training various neural network models aimed at localizing and identifying neural sources with varying characteristics and extents.

A fully connected feed-forward neural network (FCNN) was developed to tackle source localization by recognizing patterns in electrical potentials resulting from single current dipoles as well as pairs of dipoles with fixed strengths. Upon evaluation, the FCNN successfully predicted the localization of single current dipoles, although the positional error increased when tasked with simultaneous localization of two current dipoles. Expanding the FCNN's capabilities to handle current dipoles with varying strengths resulted in a reduction in localization accuracy, although the network excelled in identifying dipole strengths. The FCNN was further tasked with predicting spherical regions of active correlated current dipoles with varying radii and strengths, which showed an enhanced precision in dipole strength. All FCNN approaches provided positional errors smaller than 10 mm.

Besides the FCNN approach, a convolutional neural network (CNN) was employed to explore whether incorporating spatial information in EEG recordings as images could enhance a neural network's ability to analyze EEG data and improve source localization. The CNN architecture, as implemented in this work, exhibited a somewhat elevated mean positional error compared to the FCNN. While both network types achieved accuracy within an acceptable range, the CNN's performance was less consistent when tasked with localizing two current dipoles simultaneously, in contrast to the FCNN.

Overall, this study provides insights into the strengths and limitations of neural network approaches in EEG source localization, emphasizing the need for further investigation and fine-tuning of methods to determine the most suitable approach for tackling the EEG inverse problem.


\section{Outlook} \label{sec:outlook}
We have employed simulated EEG data using the software LFPy 2.0 to train neural networks in localizing current dipoles. Building on this foundation, an exciting opportunity presents itself: identifying the specific type of neural activity responsible for a localized current dipole. To the best of our knowledge, neural networks have not yet been harnessed to discern the neural origin of EEG signals while also localizing these neural sources.

LFPy stands out as a versatile tool. Beyond facilitating the simulation of distinct point dipoles, it enables the simulation of various types of neural activity, resulting in specific dipoles. For instance, the type of synaptic input, whether it is excitatory or inhibitory, to a population of neurons, and the synaptic input's location (apical or basal) result in different current dipoles \cite{LFPy}.  Furthermore, there is speculation that dendritic calcium spikes can be detected from EEG signals, potentially leading to exciting new possibilities for studying learning mechanisms in the human brain \cite{suzuki2017dendritic}.

To decode different types of neural activity from EEG signals, we must first comprehend how diverse neural activities are mirrored in these signals. This understanding can be gained through simulations with LFPy, which provides a platform for simulating varying neural activities and their resulting EEG signals. This, in turn, opens the door to a more comprehensive exploration of the relationship between EEG signals and the underlying neural activity, through neural networks.






% - [ ] Koden er skrevet slik at nettverket også kan trene på flere dipoler. Hadde vært interessant å studert.
% - [ ] Finne én eller flere, ikke vite hvor mange output?



% \section{Overfitting}
% L2 regularization tequniqe was employed in order to account / prevent overfitting. We did indeed not see any trend of overfitting in when looking at the loss curves. With this said, we must emphasize that there is a risk, ...




% \section{Extending DiLoc}
% % Is it true that sigmoid helpt the efficentcy of the lerning process for the network? For improvements, it would be beneficial to elaborate on the rationale behind using hyperbolic tangent for the hidden layers and ReLU for the first layer.


% \section{Multiple dipoles; thre .. }
%
% \section{Small Discussion and next chapter}
% \rednote{Performance of networks}
% \rednote{How we in next chapter will continue using Diloc and adde extentions.}
%
% \subsection{Customized Cost Function}
% \rednote{Få frem her at i stedet for å skalere target kunne vi i prinsippet ha vektet de ulike target-verdiene ulikt. Legg til at dette ble forsøkt gjort da vi få at amlitude ikke ble så optimisert, men uten hell. }
%
% In crafting our customized cost function, it is important to acknowledge that, like the built-in Mean Squared Error (MSE) cost function, our formulation inherently treats all target values equally during the optimization process. In other words, the algorithm assigns the same weight to each target value when striving to reduce the overall cost. This approach ensures that errors of equal percentage magnitude in different target values are treated on a level playing field. Consequently, a 1$\%$ error for one target value is considered as important as a 1$\%$ error for another target value, regardless of the specific range or scale of these values.
%
% It is worth noting that our choice to uniformly weight all target values is an intentional design decision. While alternative approaches, such as assigning different weights to different target values, could have been explored, we prioritize the creation of a balanced model that can accurately predict all facets of our target values. This approach stems from our objective of achieving a comprehensive understanding of EEG signal sources through a holistic and equitable modeling approach.
%
% Moreover, this uniform weighting approach aligns with our broader modeling philosophy, emphasizing the creation of a model that is versatile and adaptable across a spectrum of EEG data variations. Our aim is not only to develop a model capable of accurately predicting target values but also to ensure that its predictive capabilities are unbiased and comprehensive, covering the multifaceted aspects of EEG signal analysis.
%
% In this way, our customized cost function showcases the fusion of machine learning principles with the nuanced requirements of clinical medicine, as we strive to bridge the gap between technical prowess and real-world medical applications.
%
%
% \subsection{Metrics of success}
% To address the performance of DiLoc, we establish threshold values that represent acceptable errors for a majority of predictions. In particular, we are interested in determining the percentage of samples for which the network predicts the Euclidean distance of one or more dipoles within specific thresholds—3 mm, 5 mm, 10 mm, and 15 mm, where 3 mm is considered optimal.
%
% \rednote{This you need to ask Torbjørn....}
% \rednote{Mention/ add histograms}
% Regarding amplitude and radius predictions, the analysis involves studying the percentage of samples where the network provides predictions with absolute errors equal to 1, 2, and 3 mA$\mu$ m, and 1, 3, and 5 mm. Providing a MAE for the amplitude equal to 3 mA$\mu$ m corresponds to an error of 30$\%$,and a MAE for the radius equal to 5 mm corresponds to an error of 33$\%$, both of which are intuitively considered large errors. However, if we consider these target values in relation to potential clinical significance, a different perspective emerges. In real-world clinical cases, it could be of significant interest to discern whether a neuron source exhibits the characteristic of a small magnitude (ranging from 1 to 3 mA$\mu$ m), a medium magnitude (ranging from 3 to 6 mA$\mu$ m), or a strong magnitude (ranging from 4 to 10 mA$\mu$ m). Similarly, a small radius (ranging from 1 to 5 mm), a medium radius (ranging from 5 to 10 mm), or a large radius (ranging from 5 to 10 mm) might hold clinical significance when considering the underlying neuronal mechanisms.
%
% This shift in perspective highlights the nuanced interpretation of errors and underlines the importance of clinical context in evaluating the performance of DiLoc. In this light, even what might initially appear as substantial errors can offer valuable insights into the behavior of neuronal sources within real-world scenarios.
%
% In sum, the suite of error metrics, coupled with threshold-based assessments, facilitates an in-depth evaluation of the network's capabilities. This multi-faceted approach bridges the realms of machine learning principles and clinical applicability, encapsulating the overarching goal of achieving accurate, meaningful, and real-world clinical predictions.






\end{document}
