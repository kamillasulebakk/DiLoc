% !TEX root = main.tex
\documentclass[a4paper, UKenglish, 11pt]{uiomaster}
\usepackage{lipsum}
\usepackage[subpreambles=true]{standalone}
\usepackage{graphicx}


\begin{document}

\chapter{Method: Creating EEG Data}
In preparation for the application of neural networks to address the inverse problem, the acquisition of a substantial and appropriate EEG dataset is essential. This chapter focuses on utilizing the New York Head model in conjunction with the current dipole approximation to construct biophysically realistic EEG data.
In Section \ref{chap:simulation}, we will provide a detailed exploration of our EEG data simulation methods. Moving forward to Section 2.2, we will discuss the introduction of noise and its significance in the context of our data. Finally, in Section \ref{chap:final_data} , we will outline the characteristics of the final dataset, which will serve as the input for our neural network in solving the EEG inverse problem.


\section{Simulation of EEG Signals from Single Dipoles} \label{chap:simulation}
% The New York Head model and its  were made available for reuse in the {\tt h5py} Python package.
To create EEG data we use the New York Head model and its lead field matrix, which are integrated into the Python module LFPy 2.0 \cite{LFPy}. This software is a userfriendly wrapper around the NYHM, allowing for simulations of neural activity and the resulting EEG signals. Within LFPy, we use the \texttt{NYHeadModel} class to calculate EEG signals originating from a desired current dipole moment. For more information about the LFPy module, we refer the reader to Hagen, Næss, Ness and Einevoll (2018) \cite{LFPy}.

The implementation of the lead field matrix through the New York Head consists of 74,382 discrete points, each representing potential dipole source locations within the cortex. To sample a single data point we position a current dipole moment at one of the possible locations, and calulate its corresponding EEG signal according to the procedure outlined in Chapter \ref{chap:eeg}. The \texttt{NYHeadModel} class defines the structure for EEG data, resulting in a two-dimensional output. This output includes the values measured at each of the 231 electrodes, along with the time development of the signal for each of the electrode's measurements. To begin with, we maintain uniform magnitudes for the dipole signals. By setting these magnitudes to $10^7$ nA$\mu$m, the resulting EEG measurements span a range of approximately -1 to 1 $\mu$V.

\rednote{Kommer det tydelig nok frem hvorfor vi gjør dette?}
To ensure that the dipole orientations are predominantly aligned with the depth direction of the cortex, as emphasized in Chapter \ref{chap:eeg}, each dipole moment is rotated to be normal to the surface of the cerebral cortex. Note that aligning the dipole orientations perpendicularly to the cortex does not always lead to the dipole's normal vector pointing directly outward toward an EEG electrode. This variability arises due to the complex folding patterns found in the human cortex. In certain cases, when a dipole is located within a sulcus, the electrical signal it generates may follow the contour of the sulcus and enter deeper into the cortex. Consequently, the EEG signal must traverse a longer path before reaching an EEG electrode \cite{naess2021biophysically}.

\rednote{Kommer det tydelig nok frem hvordan man i EEG leter etter frekvensbånd? Fourier - wavelet(?). Jo lenger signal jo bedre fourieranalyse. Fyll på info. Extracting diverse frequency spectra and analyzing time series to identify anomalies.}
\rednote{Can I find this in any books? Ask Torbjørn.}


In the preceding chapter, we discussed how EEG analysis often revolves around the investigation of specific frequency bands. However, an alternative and widely recognized method for studying EEG measures involves the practice of averaging the EEG responses to a specific stimulus across multiple trials. This technique helps reduce noise and allows us to pinpoint the time step within the EEG time series where the averaged signal reaches its peak magnitude. As a result, it provides a 'static' EEG signal with reduced noise. The underlying principle for this approach is the quasi-static approximation, which posits that the potential measured from a dipole propagates instantaneously from the source to the electrode. This simplification is based on the assumption that the EEG signal at any point effectively captures a snapshot of the dipole activity within the brain. This means that if a dipole exhibits oscillations at a particular frequency, the corresponding EEG signal will oscillate at the same frequency, without phase shifts.

In our analysis, we sample the EEG data to obtain what we may consider an arbitrary time step, effectively representing a static snapshot of the electrode recordings of a current dipole source with magnitude $10^7$ nA$\mu$m. This alternative to the traditional time series data offers simplification and computational efficiency without significant deviation from standard clinical EEG analysis. This construction is further justified by the fact that our simulated data are noise-free, eliminating the risk of noise interference with the EEG samples. Consequenscly, we are left with one-dimensional EEG signals mirroring the dipole strengths and locations within the cerebral cortex.


\section{Noise} \label{chap:noise}
Experimental EEG recordings inevitably contain noise, which can disrupt the accurate analysis of brain activity. \emph{Artifacts}, which are signals recorded by EEG but originating from sources other than neuronal communication, pose a particular challenge in real-world data. Some artifacts can mimic genuine epileptiform abnormalities or seizures, underscoring the importance of identifying and distinguishing them from true brain waves \cite{sazgar2019eeg}.

Artifacts can be classified into two categories based on their origin. \emph{Physiological artifacts} arise from the patient's own physiological processes, including ocular activity, muscle activity, cardiac activity, perspiration, and respiration. \emph{Technical artifacts}, on the other hand, are electromagnetic interferences from external factors such as cable and body movements \cite{bitbrain}.

Filtering techniques are commonly employed to remove artifacts from EEG recordings prior to analysis. However, in the case of simulated EEG data, the need for artifact removal is eliminated as the data inherently does not contain noise. Simulated EEG data can be considered as filtered and preprocessed, ensuring a high signal-to-noise ratio (SNR) \cite{wiki-snr}. Nevertheless, to ensure that the data aligns with real-world scenarios, it is necessary to introduce noise to the data before feeding it into the neural network.

In our approach, we recognize that the introduction of noise to the simulated EEG data is an essential step to enhance the robustness of the trained neural network and ensure its ability to handle real EEG recordings effectively. Since the specific characteristics and quantity of noise have not been the primary focus of our study, we have opted for a straightforward approach. Our final dataset incorporates normally distributed noise with a mean of 0 and a standard deviation equal to 10$\%$ of the standard deviation observed in the simulated EEG recordings. Adding this noise gives random variations around each data point while preserving the overall normalization properties of the dataset.

\begin{figure}[!htb]
    \centering
    \hspace*{-3cm}
    \includegraphics[width=18cm]{figures/simple/simple_example.pdf}
    \caption{EEG measurements for a sample containing one single current dipole source at an arbitrary position within the celebral cortex. Noise is added to the EEG signal. The EEG measure is seen from both sides ($xz$-plane and $yz$-plane) and above (the $xy$-plane). EEG electrode locations are presented as filled circels, where the color of the fill represents the magnitude of the measured signal for the given electrode. The position of the current dipole moment is marked with a yellow star.}
    \label{fig:eeg_field_1_dipole_example}
\end{figure}

\section{Final Dataset} \label{chap:final_data}
The final dataset comprises 70,000 samples, where each sample holds 231 values representing the EEG measurements recorded at each electrode - a configuration directly inherited from the NYHM. Target values for the EEG samples are the $x$-, $y$- and $z$-coordinates of the different dipole sources.

Figure \ref{fig:eeg_field_1_dipole_example} presents an example of the input EEG data for a single sample, with added noise. The prominent dipolar pattern in the figure indicates that the dipole is located within a sulcus. In the figure the EEG measure is visualized from multiple perspectives: the $xz$-plane, $yz$-plane, and the $xy$-plane. The electrode locations are represented by filled circles, with the color of the fill indicating the magnitude of the measured signal at each electrode. The position of the current dipole moment is denoted by a yellow star. As indicated by the colorbar in the figure, the EEG signal for the specific sample ranges from -1 to 1 $\mu$V, which is the range that the simulated EEG data for all samples will fall within.



% Before being feed to the DiLoc network for training, the data is splitted into train, validation and test parts. The train- and validation data are the batches of the data set that the network uses during training. Out of the 70 000 samples in the final dateset, 50000 is set off to the purpose of train and validation data. Out of these 50000 sampes, randomly selected 80 percent of the rows are put into the training set. The remaining 20 percent operates as the validation set, which is useful in order to prevent the network to overfit during training. The test set which contains the final 20 000 samples will be used after the training prosess for the purpose of testing how well the model generalizes to new, unseen data.
%
% Prior to being fed into the DiLoc network for training, the dataset was splittied into distinct segments: the train, validation, and test sets. This partitioning is vital for assessing and optimizing the network's performance. Among the 70 000 samples in the final dataset, 50 000 samples are designated for the train and validation data. To ensure a representative and unbiased allocation, 80 percent of these 50 000 samples are randomly assigned to the training set. This training set serves as the core data that the network utilizes during the training process. The remaining 20 percent of the 50 000 samples form the validation set. This set plays the role in preventing overfitting, the phenomenon where the network becomes excessively attuned to the training data and consequently performs poorly on new data. By independently evaluating the model's performance on the validation set throughout training, we can fine-tune the network's parameters to achieve better generalization to unseen data. Once the network completes its training process, the test set comes into play. Comprising 20 000 samples, the test set serves as the benchmark for assessing the model's ability to generalize and make accurate predictions on new data instances. By adhering to this rigorous train-validation-test data partitioning, we ensure a robust evaluation of the DiLoc model's performance and its capacity to effectively handle real-world scenarios with previously unseen data.



\end{document}