\documentclass[a4paper, UKenglish, 11pt]{uiomaster}
\usepackage{lipsum}
\usepackage[subpreambles=true]{standalone}
\usepackage{graphicx}


\begin{document}
\chapter{EEG inverse problem: Machine Learning approaches}
Machine learning is a field concerned with constructing computer programs that learn from experience, where the utialization of data improves computer performance across various tasks. Within this broad scope, one notable application lies in the identification of sources generating abnormal electrical brain signals. By employing specific machine learning algorithms, EEG data can be processed and analyzed to accurately localize the sources responsible for the recorded signals. These algorithms learn from the data and uncover patterns that associate the signals with their corresponding sources, effectively solving the EEG inverse problem. In this chapter, we introduce the field of machine learning and provide an overview of relevant tequniqes for solving our specific EEG inverse problem and its wider implications.

\section{Machine Learning and Neural Networks}
% Include bias variance
% Include SGD, momentum
% Training and test data

"Machine Learning is a subfield of artificial intelligence with the goal of developing algorithms capable of learning from
data automatically" \cite{mehta2019high}. The typical machine learning (ML) problems are addressed using the same three elements. The first element is the dataset $\mathcal{D} = (\textbf{X}, \textbf{y})$ where $\textbf{X}$ commonly is refered to as the design matrix, and consists of independent variables, and $\textbf{y}$ is a vector consisting of dependent variables. Next, we have the model itself, $f(\textbf{x}; \boldsymbol{\theta})$. The ML model can be seen as a function used to predict an output from a vector of input variables, i.e. $f : \textbf{x} \rightarrow y$ of the parameters $\boldsymbol{\theta}$. Finally, the third element, allows us to evaluate how well the model performs on the obervations $\textbf{y}$. This element is known as the cost funtion $\mathcal{C}(\textbf{y}, f(\textbf{X}); \boldsymbol{\theta})$.

\subsection{Fitting a Machine Learning Model}
The first step in "fitting" a machine learning model, is to randomly split the dataset $\mathcal{D}$ into train and test sets. This is done in order to make a model compativle with multiple data sets. The size of each set commonly depend on the size of the data set avaible, however a rule of thumb is that the majority of the data are partitioned into the trainng set (e.g., 80$\%$) with the remainder going into the test set \cite{mehta2019high}.

When using the expression "fitting a model" one commonly refer to finding the value of $\boldsymbol{\theta}$ that minimizes a chosen cost function, employing data from the training set. One commonly used cost funtion is the squared error, in which can be written as follows:

\begin{equation}
\text{MSE}(\boldsymbol{\theta}) = \frac{1}{n}
\sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2 ,
\label{eq:MSE}
%MSE(\textbf{y},\mathbf{\tilde{y}}) =
\end{equation}

where $\boldsymbol{\theta} = \theta_0, \theta_1, ..., \theta_n$ denotes the model parametes, $\tilde{y}_i$ represents the predicted value and $y_i$ is the corresponding true value.

A general expression for any type of cost function can be formulated as follows:

\begin{equation}
C(\theta) = \Sigma^n_{i=0}c_i(\textbf{x}_i, \theta)
\end{equation}

In this expression, $c_i(\textbf{x}_i, \theta)$ represents the cost associated with the $i$-th data point, where $\textbf{x}_i$ represents the input data and $\theta$ denotes the parameter vector. This notation emphasizes the summation over all data points from 1 to $n$, where each data point contributes its own cost to the overall cost function.

In order to minimize the cost function and find the optimal values for the model parameters, $\boldsymbol{\theta}$, an optimization alorithm is typically employed. One widely used optimization algorithm is gradient descent, which iteratively updates the parameters based on the negative gradient of the cost function.

\subsection {Gradient Descent and Its Variants}

Gradient Descent (GD) is an iterative optimization algorithm used to locate a local minima of a differentiable function. The core concept of the algorithm is based on the observation that a function $F(\textbf{x})$ will decrease most rapidly if we repeatedly move in one direction opposite to the negative gradient of the function at a given point $\textbf{w}$, $-\nabla F(\textbf{a})$. This means that if

\begin{equation}
\textbf{w}_{n+1} = \textbf{w}_n - \eta\nabla F(\textbf{w}_n)
\end{equation}

for a sufficiently small learning rate $\eta$, we are always moving towars a minimum, since $F(\textbf(w)_n) \ge F(\textbf(w)_{n+1})$ \cite{wiki-gradient-descent}. After each update, the gradient is recalculated for the updated weight vector $\textbf{w}$, and the process is repeated \cite{bishop2006pattern}. Based on this observation, the iterative process begins with an initial guess $x_0$ for a local minimum of the function $F$. It then generates a sequence $\textbf{x}_0, \textbf{x}_1, \textbf{x}_2, ..., \textbf{x}_n$ such that each element in the sequence is upated according to the rule:

\begin{equation}
\textbf{x}_{n+1} = \textbf{x}_{n} - \eta_n\nabla F(\textbf{x}_n), n \ge 0,
\end{equation}

where $\eta_n \ge 0$. The sequence forms what we call a monotonically decreasing sequence:

\begin{equation}
F(\textbf{x}_0) \ge F(\textbf{x}_1) \ge F(\textbf{x}_2) \ge ... \ge F(\textbf{x}_n)
\end{equation}

Hence, with this iterative process, it is hoped that the sequence $(\textbf{x}_n)$ converges to the desired local minimum \cite{wiki-gradient-descent}.

However, it is important to note that the error function in gradient descent is computed based on the training set, so that each step requires that the entire training set, reffered to as the \emph{batch}, is processed in order to evaluate the new gradient. In that sense, gradient descent is generally considered a suboptimal algorithm. This perception aligns with the algorithms sensitivity to the initial condition, $\textbf{w}_0$, and the choice of the learning rate $\eta$. The sensiticity to initial conditions can be explained by the fact that we to a large extent most often deal with high-dimensional, non-convex cost functions with numerous local minima - where the risk of getting stuck in local minimums if the initial guess is not accurate. Additionally, guessing on a too large learning rate may result in overshooting the global minimum, leading to unpredictable behavior, while a too small learning rate increases the number of iterations required to reach a minimum point, thereby increasing computational time. Stochastic gradient descent, however, is a version of gradient descent that has provided useful in practise for training machine learning algorithms on large data sets \cite{bishop2006pattern}.

\subsubsection{Stochastic Gradient Descent}
The method of Stochastic Gradient Descent (SGD) allows us to compute the gradient by randomly selecting subsets of the data at each iteration, rather than using the entire dataset \cite{bishop2006pattern}. The update can be written as:

\begin{equation}
\textbf{w}_{\tau+1} = \textbf{w}_{\tau} - \eta\nabla F_n(\textbf{w}_\tau)
\end{equation}

These smaller subsets taken from the entire dataset are commonly reffered to as mini-batches. In other words, SGD is just like regular GD, except it only looks at one mini-batch for each step. Introducing fluctuation by only taking the gradient on a subset of the data, is beneficial as it enables the algorithm to jump to a new and potentially better local minima, rather that getting stuck in a local minimum point.

\subsubsection{Stochastic Gradient Descent with Momentum}
Splitting the dataset into mini-batches, as done with SGD, naturally reduces the calculation time. However, adding \emph{momentum}, to the algorithm, not only leads to faster converging, due to stronger acceleration of the gradient vectors in the relevant directions, but also improves the algorithms sensitivity to initial guess of the learning rate $\eta$. The momentum can be understood as a memory of the direction of the movement in parameter space, which is done by adding a fraction $\gamma$ of the weight vector of the past time step to the current weight vector:

\begin{equation}
\textbf{v}_{\tau} = \gamma\textbf{v}_{\tau-1} - \eta\nabla F_n(\textbf{w}_{\tau})
\end{equation}

\begin{equation}
\textbf{w}_{\tau} = \textbf{w}_{\tau-1} + \textbf{v}_{\tau}
\end{equation}

Here, $\textbf{w}{\tau}$ represents the updated weight vector at iteration $\tau$, $\textbf{w}_{\tau-1}$ is the previous weight vector, $\textbf{v}_{\tau}$ is the updated momentum vector at iteration $\tau$, $\gamma$ is the momentum coefficient, $\eta$ is the learning rate, and $\nabla F_n(\textbf{w}_{\tau})$ is the gradient of the cost function $F_n$ computed on the mini-batch.

% A general expression for any type of cost function can be formulated as follows:
%
% \begin{equation}
% C(\theta) = \Sigma^n_{i=0}c_i(\textbf{x}_i, \theta)
% \end{equation}
%
% In this expression, $c_i(\textbf{x}_i, \theta)$ represents the cost associated with the $i$-th data point, where $\textbf{x}_i$ represents the input data and $\theta$ denotes the parameter vector. This notation emphasizes the summation over all data points from 1 to $n$, where each data point contributes its own cost to the overall cost function.
%
% From the general expression it falls out that the total gradient can be computed as a sum over i gradients:
%
% \begin{equation}
% \nabla_{\theta}C(\theta) = \Sigma^n_{i=0}\nabla_{\theta}c_i(\textbf{x}_i \theta).
% \end{equation}

%When the mean squared error (MSE) is zero, it indicates that the predicted values $\tilde{y}_i$ match the true values $y_i$ with perfect accuracy. This level of accuracy is ideal, but typically not attainable in practice.
%Having a design matrix $\textbf{X}$ of size $N \cross p$, where N refers to the total number of samples, and $p$ is the number of features for each sample, we can denote the



\subsection{Neural Networks}
Neural networks are a distinct class of nonlinear machine learning models capable of learning tasks by observing examples, without requiring explicit task-specific rules \cite{Hjorth-Jensen2022}. The models mimics the way bilogical neurons trasmit signals, with interconnected nodes known as neurons that communicate through mathematical functions across layers. The layers in neural networks contain an arbitraty number of neurons, where each connection is represented by a weight variable.

The behaviour of the human brain has inspired the following simple mathematical model for an artificial neuron:

\begin{equation}
  a = f \left( \Sigma_{i=1}^n w_ix_i \right) ) = f(z)
\label{eq:neuron}
\end{equation}

where $a$ is the output of the neuron, and is the value of the neurons activation function $f$ which has as input a weighted sum of signals $x_i, x_{i+1},...,x_n$ recieved by $n$ other neurons, multiplied with the weights $w_i, w_{i+1}, ..., w_{n}$ and added with bieases $b_i, b_{i+1}, ..., b_{n}$.  A neural network consits of many such neurons stacked into layers, with the output of one layer serving as the input for the next. Typically, the neural networks built up of an input layer, an output layer and layers in between, called hidden layers.

The exact function $a$ varies depending on the type of non-linearity that exists in the activation function applied to the input of each neuron. However, in almost all cases $a$ can be decomposed into a linear operation that weights the relative importance of the various inputs, and a non-linear transformation $f(z)$ which is usually the same for all neurons in the network. As seen in equation \ref{eq:neuron}, the linear tranformation commonly takes the form of a dot product with a set of neuron-specific weights followed by re-centering with a neuron-specific bias. A more convenient notation for the linear transformation $z^{i}$ then goes as follows:

\begin{equation}
z^{i} = \boldsymbol{w}^{(i)} \cdot \boldsymbol{x} + b^{(i)} = \mathbf{x}^T \cdot \mathbf{w}^{(i)} ,
\label{eq:linear_transformation}
\end{equation}


where $\mathbf{x} = (1, \boldsymbol{x})$ and $\mathbf{w}^i = (b^{(i)}), \boldsymbol{w}^{(i)})$. The full input-output function can be expressed by incorporating this into the non-linear activation function $a_i$, as expressed below.

\begin{equation}
a_i(\mathbf{x}) = f_i(z^{(i)}) .
\label{eq:linear_transformation}
\end{equation}

In figure \ref{fig:NN_basic_architecture} we have provided the basic architecture of neural networks. Here nodes are depiced as circular shapes, while arrows indicate connections between the nodes.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/basic_architecture.png}
    \caption{$\textbf{(A)}$ The fundamental structure of neural networks comprises simplified neuron units that perform a linear operation to assign different weights to inputs, followed by a non-linear activation function.$\textbf{(B)}$ These neuron units are organized into layers, where the output of one layer serves as the input to the subsequent layer, forming a hierarchical arrangement.}
    \label{fig:NN_basic_architecture}
\end{figure}


\subsubsection{Feed Forward Neural Networks}
The feedforward neural network (FFNN) was one of the first artificial neural network to be adopted and is yet today an important algorithm used in machine learning. The feed forward neural network is the simplest form of neural network, as information is only processed forward, from the input nodes, through the hidden nodes and to the output nodes \cite{Hjorth-Jensen2022}.


\subsubsection{Convolutional Neural Networks}
Convolutional neural networks (CNNs) is an other variant of FFNNs that have drawen inspiration from the functioning of the visual cortex of the brain. In the visual cortex, individual neurons exhibit selective responses to stimuli within small sub-regions of the visual field, known as receptive fields. This property allows the neurons to effectively exploit the spatially local correlations present in natural images. Mathematically, the response of each neuron can be approximated using a convolution operation \cite{Hjorth-Jensen2022}.

% TODO: check sources
CNNs mimic the behavior of visual cortex neurons by utializing a specific connectivity pattern between nodes in adjacent layers. Unlike fully contected FFNNs, where each node connects to all nodes in the preceding layer, CNNs  local connectivity. In other words, each node in a convolutional layer is only connected to a subset of nodes in the previous layer. Typically, CNNs consist of multiple convolutional layers that learn local features from the input data. These layers are followed by a fully connected layer that combines the learned local information to produce the final outputs. CNNs find wide applications in image and video recognition tasks \cite{Hjorth-Jensen2022}.

\subsection{Activation functions}
The output of a neural network will always be a linear function of the inputs. By introducing  non-linearities into the neural networks computations, activation functions makes the network able to capture complex relationships and patterns that cannot be represented by simple linear functions.

Activation functions in neural networks draw inspiration from the behavior of neurons in the brain. Similar to how neurons respond to incoming electrical signals, activation functions determine whether a neuron in a neural network should be activated or not based on the strength of the input. If the input exceeds a certain threshold, the neuron "fires" or becomes activated, otherwise it remains inactive. This behavior mimics the action potential of neurons in the brain, allowing neural networks to model complex decision-making processes and nonlinear relationships in data.


\subsubsection{Sigmoid}
The sigmoid activation function is one of the more biologically plausible as the putput of inactivated neurons returns zero \cite{Jensen2022}. 

\subsubsection{Hyperbolig Tengent}

\subsubsection{Rectified Linear Unit}


\subsubsection{Back propagation algorithm}
The back propagation algorithm is a clever procedure that allows us to change the weights in order to minimize the cost function. At its core, back propagation is simply the ordinary chain rule for partial differentiation, and can be summarized using four equations \cite{mehta2019high}.

The fist equation is the definition of the error $\delta_i^L$ of the $i$-th neuron in the $L$-th layer:

\begin{equation}
    \delta_i^L = \frac{\partial C}{\partial(z_i^L)},
\label{eq:I}
\end{equation}
which can be thought of as the change to the cost function by increasing $z_i^L$ infinitesimally. By definition, the cost function measures the error of the output compared to the target data. So if the error $\delta_i^L$ is large, that would suggest the cost function hasn't yet reached its minima.
The second equation is the analogously defined error of neuron $i$ in layer $l$, $\delta_i^l$:
\begin{equation}
    \delta_i^l = f'(z_i^l)\frac{\partial C}{\partial(a_i^l)}
\label{eq:II}
\end{equation}
where $f'(z_i^l)$ measures how fast the activation function $f$ is changing at the given activation value.
Utilizing that $\delta_i^l = \frac{\partial C}{\partial z_j^l}$ we can express the error in terms of the equations for layer $l+1$. This can be done by using the chain rule, and is the third back propagation equation (for full calculations see ...):

\begin{equation}
    \delta_i^l = \frac{\partial C}{\partial z^l_{i}} = \Sigma \delta_j^{l+1}w_{ij}^{l+1}f'(z_i^l)
\label{eq:III}
\end{equation}

Finally the last equation of the four back propagation equations the derivative of the cost function in terms of the weights:

\begin{equation}
    \frac{\partial C}{\partial w^l_{ij}} = \delta_i^l a_j^{l-1}
\label{eq:IV}
\end{equation}

With these four equations in hand we can now calculate the gradient of the cost function, starting from the output layer, and calculating the error of each layer backwards. We then have a way of adjusting all the weights and biases to better fit the target data. The back propagation algorithm then goes as follows:

\begin{enumerate}
  \item \textbf{Activation at input layer:} calculate the activations $a_i^1$ of all the neurons in the input layer.
  \item \textbf{Feed forward:} starting with the first layer, utilize the feed-forward algorithm through \ref{eq:FFNN} to compute $z^{l}$ and $a^{l}$ for each subsequent layer.
  \item \textbf{Error at top layer:} calculate the error of the top layer using equation \ref{eq:I}. This requires to know the expression for the derivative of both the cost function $C(\boldsymbol{W}) = C(\boldsymbol{a}^L)$ and the activation function $f(z)$.
  \item \textbf{"Backpropagate" the error:} use equation \ref{eq:III} to propagate the error backwards and calculate $\delta_j^l$ for all layers.
  \item \textbf{Calculate gradient:} use equation \ref{eq:II} and \ref{eq:IV} to calculate $\frac{\partial C}{\partial z^l_{i}}$ and $\frac{\partial C}{\partial w^l_{ij}} = \delta_i^l a_j^{l-1}$. \newline
  \item \textbf{Update weights and biases:} \\[2pt] $w^l_{jk}=w^l_{jk}-\eta\delta^l_ja^{l-1}_k$ \\[2pt] $b_j^l = b_j^l - \eta \delta_j^l$
\end{enumerate}










% notethis
Computational neuroscience is a field that aims to understand the principles underlying information processing in the brain using mathematical and computational tools. The inverse problem in EEG, which involves estimating the location and strength of electrical sources in the brain based on measurements of electrical activity on the scalp, is a key challenge in computational neuroscience. Machine learning techniques, including feedforward neural networks, have been used to address this problem by learning to map the measured EEG signals to estimates of the underlying electrical sources in the brain.

Source localization using machine learning techniques has shown promise for improving the accuracy and efficiency of EEG analysis, and has been applied to a variety of cognitive and clinical applications. For example, machine learning-based source localization has been used to study the neural mechanisms underlying attention, memory, and perception (Wu et al., 2018; Lopes da Silva et al., 2019), as well as to diagnose and monitor neurological disorders such as epilepsy (Safieddine et al., 2019; Shah et al., 2020). These applications demonstrate the potential of machine learning and computational neuroscience to enhance our understanding of the brain and improve clinical outcomes.

Machine learning is a field of computer science that involves using algorithms and statistical models to enable computers to learn from data without being explicitly programmed. One popular type of machine learning algorithm is the feedforward neural network, which is a type of artificial neural network that is often used for tasks such as linear regression. In a feedforward neural network, data is passed through a series of layers of interconnected nodes, or "neurons," which perform mathematical operations to transform the data.

Linear regression is a common machine learning task that involves predicting a continuous quantity, such as the price of a house or the temperature of a city, based on a set of input features. In a feedforward neural network, linear regression can be accomplished by using a single neuron in the output layer of the network that computes a weighted sum of the input features and applies an activation function to produce the predicted output value. The weights on the input features are learned by the network during the training process, which involves adjusting the weights to minimize the difference between the predicted output values and the actual output values in the training data.

Overall, feedforward neural networks are a powerful machine learning tool that can be used to solve a wide range of problems, including linear regression. By adjusting the weights and biases of the neurons in the network during the training process, neural networks can learn to make accurate predictions based on input data, making them a valuable tool for a variety of applications.

% References:
%
% Lopes da Silva, F. H., Da Silva, F. L., Blanes, W., & Kalitzin, S. N. (2019). Towards a functional definition of epileptic networks. Epilepsy & behavior, 102, 106643.
%
% Safieddine, D., Murai, K. K., Tsakalis, K. S., & Valiante, T. A. (2019). Dynamic source localization of epileptic spikes using a recurrent neural network. Frontiers in neuroscience, 13, 1352.
%
% Shah, P., Taylor, P. N., & Worrell, G. A. (2020). Automatic detection and localization of seizures in intracranial electroencephalographic recordings using deep learning. Journal of neural engineering, 17(1), 016015.
%
% Wu, T., Li, H., He, B., & Li, Y. (2018). A review of techniques for detecting and localizing EEG abnormalities. Journal of neuroscience methods, 302, 44-57.




​





% \section*{Machine Leaning}
% The fit is found using the method of least squares using polynomials x and y up to the eighth order. The mean square error (MSE) can be used to evaluate how well the model fit our data, which is given by:
%
% \begin{equation}
% MSE(\hat{y},\hat{\tilde{y}}) = \frac{1}{n}
% \sum_{i=0}^{n-1}(y_i-\tilde{y}_i)^2
% \label{eq:MSE}
% \end{equation}
%
% Where $\tilde{y}_i$ is the predicted value and $y_i$ is the corresponding true value. Having a MSE of zero, would mean that the estimator $\hat{y}$ predicts observations of the parameter $\hat{\tilde{y}}$ with perfect accuracy. This is obviously ideal, but is however, not typically possible.
%
% The predicted value $\tilde{y}_i$ can be rewritten as $\tilde{y}_i=x_i^T\beta$, which we can use to find the residual sum of the squares:
%
% \begin{equation}
% RSS(\beta) =
% \sum_{i=1}^{N}(y_i-x_i^T\beta)^2
% \label{eq:RSS}
% \end{equation}
%
% The ordinary least squares method (OLS) wants to minimize the sum of the residuals, which is given by the RSS value (see equation \ref{eq:RSS}). We want to find the $\beta$ which minimize this function. If we define a $N \times p$ matrix $\textbf{X}$ with each row as an input vector and a N-vector $\textbf{y}$ of the true values, we can represent our solution $\hat{\beta}$ as a 1-dimensional array of size p using matrix notation:
%
% \begin{equation}
% \hat{\beta}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\textbf{y}
% \label{eq:beta}
% \end{equation}
%
% where $\textbf{X}^T\textbf{X}$ is non singular \cite{94}. The size of $\hat{\beta}$ will depend on the degree of our polynomial fit. For example polynomial degree 5 we will give us a $\hat{\beta}$ length of p=21. The length N is given by the number of datapoints. % remember to plug in the equation for finding the length of the beta-vector
%
% Just like MSE, the coefficient of determination $R^2$ is a measure for how precise a model is. To be more precise, it provides a measure of how well future samples are likely to be predicted by the model. The score normally ranges between 0 and 1, where 1 is the best possible score. In general, the higher the R-squared, the better the model fits the data. The coefficient of determination $R^2$ is given as follows:
% %skrive litt mer om r2-score?
% \begin{equation}
% R^2(\hat{y}, \tilde{\hat{y}}) = 1 - \frac{\sum_{i=0}^{n - 1} (y_i - \tilde{y}_i)^2}{\sum_{i=0}^{n - 1} (y_i - \bar{y})^2},
% \label{eq:R2}
% \end{equation}
%
% Where the mean value of $y_i$ is defined by $\bar{y}$:
%
% \begin{equation*}
% \bar{y} =  \frac{1}{n} \sum_{i=0}^{n - 1} y_i.
% \label{eq:ybar}
% \end{equation*}
%
% Creating a perfect model for a set of data, can easily be done by using a high polynomial degree when fitting. However, creating a foolproof model that only fits one specific data set is not very beneficial. We want our model to predict correlated data, not slavishly follow the path of least square error. With this in mind we should split our data into training and testing sets in order to make the model compatible with multiple data sets. By splitting our data we can test how well our model predicts. In this exercise we have chosen to train on $80\%$ of our data, and test on the remaining $20\%$. By training, in this context, we mean to find the $\beta$ that minimize the cost function, using the training data. Having obtained $\hat{\beta}$ we test our model on both sets, expecting that it fits well for both the training and test data.
%
% A much used approach before starting to train the data is to scale our data. The data may be very sensitive to extreme values, and scaling the data could render our inputs more suitable for the algorithms we want to use.
%
% There are several methods for scaling data sets. In this project we have scaled our data by calculating the mean and standard deviation for each feature. For each observed value of the feature, we subtract the mean and divide by the standard deviation. The first feature (first column in our design matrix) was kept untouched, in order to avoid singularities when calculating $\beta$. This sort of scaling is called “Standardizing”. When standardizing data one obtain a "standard normal" random variable with mean 0 and standard deviation 1 \cite{96}.
%
% If any, there are very few data sets in the real world which have ideal properties without noise. When generating our own data set for the Franke function we therefore explore the addition of an added normal distributed $N(0,1)$ stochastic noise, in order to make the data set more realistic.
%
% \subsection*{Gradient Descent}
%
% When training an algorithm one wants to minimize the cost function to reach its global minimum. Instead of using matrix inversion to reach this minimum we now want to use gradient descent. This is useful due to the fact that it sometimes is hard to perform matrix inversion on data sets.
%
% Gradient Descent is an optimization algorithm for finding a local minimum of a derivable function. The underlying idea of the algorithm is that a function $F(\textbf{x})$ decreases fastest if we take repeated steps in the opposite direction of the negative gradient $-\nabla F(\textbf{x})$ of the function at $\textbf{x}$. We have that:
%
% \begin{equation}
%     \textbf{w}_{k+1} = \textbf{w}_k - \eta_k\nabla F(\textbf{w}_k)
%     \label{eq:GD}
% \end{equation}
%
% which leads to $F(\textbf{w}_{k+1}) \leq F(\textbf{w}_{k})$, when $\eta_k > 0$. This means that for sufficiently small step lengths $\eta_0$ we are always moving towards a minimum. In the Gradient Descent method we do an initial guess on weights $\textbf{w}_0$ and compute new approximations according to equation \ref{eq:GD}.
%
% Gradient descent then starts at a point and takes steps in the steepest downside direction until it reaches the point where the cost function is as small as possible.
%
% It is important to note that GD is very sensitive to the chosen initial condition. In machine learning we often deal with non-convex high dimensional cost functions with many local minima. This means, that unless we have a very good initial guess, we risk getting stuck in a local minimum when the gradient converges. Moreover, the algorithm is sensitive to the step length $\eta_0$, also referred to as the learning rate. If we take too large steps, we risk stepping over the global minimum point, resulting in unpredictable behavior. The step length also needs to be large enough so we don't get "stuck" in a local minimum point. Another fact is that a small learning rate will require many iterations before we reach a minimum point, which increases CPU time.
%
% One common problem with the GD method is that it has a tendency to overshoot the exact minimum. It has been shown that by slowly decreasing the learning rate we will obtain convergence behaviour similar to the batch gradient descent, which can help it converge towards a local or global minimum point \cite{100}. We define a learning rate function which adjust as a function of a variable t, which is proportional to the number of epochs. The learning rate function is given by:
%
% \begin{equation}
%     \eta(t)=\frac{t_0}{t+t_1}
% \label{eq:learning_rate_func}
% \end{equation}
%
% Where $t_0$ and $t_1$ are constants. This equation will decrease as we run through our batches, since we are dividing by the t term. In Scikit-Learn's "SGDRegressor" we have that $t_1=\frac{1}{0.01*t_0}$, which we use in our own model when we compare with the method provided in Scikit-Learn.
%
% %Furthermore, we mark that gradient as a function of $\textbf{x} = (x_1, x_2, ..., x_n)$ which makes it expensive to compute numerically. The method named Stochastic Gradient descent alleviates some of these problems by introducing randomness.
%
% \subsubsection*{Stochastic Gradient Descent with momentum}
%
% In almost all cases we can write our cost function as a sum over $n$ data points:
%
% \begin{equation}
%     C(\beta) = \Sigma^{n}_{i=1}c_i(\textbf{x}_i, \beta).
% \end{equation}
%
% Which means that the total gradient can be computed as a sum over i -gradients:
%
% \begin{equation}
%     \nabla_\beta C(\beta) = \Sigma^n_i\nabla_\beta c_i(\textbf{x}_i, \beta)
%     \label{eq:SGD}
% \end{equation}
%
% % Checkpoint
%
% The SGD method allows us to take the gradient on randomly selected subsets of data at every step rather than the full data set. These subsets are commonly referred to as mini-batches. In other words, Stochastic Gradient Descent is just like regular Gradient Descent, except it only looks at one mini-batch for each step. The mini-batches are denoted by $B_k$ where k runs from 1 and up to the number of batches, $n/M$. An iteration over the number of mini-batches is called an epoch.
%
% Introducing randomness by only taking the gradient on a subset of the data, is beneficial as it lowers the chance of getting stuck in a local minimum point. Moreover, splitting the data points into batches reduces the time spent calculating the derivatives of the cost function, since we sum over $k$ batches and not all $n$ data points.
% Implementing momentum to the Stochastic Gradient Descent, helps accelerate gradient vectors in the right directions, which leads to faster converging. In many ways this improves the algorithms sensitivity to the learning rate. The momentum serves as a memory of the direction we are moving in parameter space and can be written as follows:
%
% \begin{equation}
%     \textbf{v}_{t} = \eta\textbf{v}_{t-1} - \eta_t\nabla F(\boldsymbol{\beta}_t)
%     \label{eq:GDM}
% \end{equation}
%
% \begin{equation}
%     \boldsymbol{\beta}_{t+1} = \boldsymbol{\beta}_t - \textbf{v}_t
%     \label{eq:GDM_beta}
% \end{equation}
%
% where the parameter $\eta$ represents the momentum and must be between 0 and 1. The momentum term $\eta$ is usually set to 0.9 or a similar value \cite{100}. We used $\eta = 0.9$. We also set the initial "velocity" to $v=0$.
%
% \subsubsection*{The momentum SGD algorithm}
% \begin{algorithmic}[H]
% \label{sudo:MSGD}
% \\
% \State \textit{Initialize all the parameters.}
% \State \textit{Create design matrix $X$}
% \State \textit{Call Franke's Function with $x$ and $y$. This is our $z$.}
% \State \textit{Guess on some $\beta$ values.}
% \For { $epoch \in \{N_{epochs}\}$}
% \State \textit{Shuffle the data}
% \For { $index \in \{m_{minibatches}\}$}
% \State \textit{Calculate gradients}
% \State \textit{Find current learning rate}
% \State \textit{$v = \eta * v + \eta *$ gradients}
% \State \textit{$\beta = \beta - v$}
%
% \end{algorithmic}
% \\
% For the ordinary SGD method we have one less term in the loop over minibatches: \textit{$\beta = \beta - \eta * gradients$}.
%
%
% \subsection*{Neural  Networks}
% %Gradient descent is the preferred way to optimize neural networks.
% Artificial Neural Networks are computational systems that can learn to perform tasks by considering examples, generally without being programmed with any task-specific rules \cite{101}.
%
% The biological neural networks of animal brains, wherein neurons interact by sending signals in the form of mathematical functions between layers, has inspired a simple model for an artificial neuron:
%
% \begin{equation*}
%     a = f \left( \Sigma_{i=1}^n w_ix_i + b_i\right ) = f(z)
%     \label{eq:NN}
% \end{equation*}
%
% where the output $a$ of the neuron is the value of its activation function $f$, which as input has the sum of signals $x_i, x_{i+1}, ..., x_n$ received by $n$ other neurons, multiplied with the weights $w_i, w_{i+1}, ..., w_n$ and added with biases.
%
% Most artificial neural networks consists of an input layer, an output layer and layers in between, called hidden layers. The layers consists of an arbitrary number of neurons, also referred to as nodes. The connection between two nodes is associated with a weight variable $w$, that weights the importance of various inputs. A more convenient notation for the activation function is:
%
% \begin{equation}
%     a_i(\boldsymbol{x}) = f_i(z^{(i)}) = f_i(\boldsymbol{w^{i}}\cdot \boldsymbol{x} + b^{i})
% \label{eq:NN_vec}
% \end{equation}
%
% where $\boldsymbol{w}^{(i)} = (w_1^{(i)}, w_2^{(i)}, ..., w_n^{(i)})$ and $b^{(i)}$ are the neuron-specific weights and biases respectively. The bias is normally needed in case of zero activation weights or inputs \cite{101}.
%
% \subsubsection*{Feed Forward Neural Network}
%
% As the name name suggests, in Feed Forward Neural Network (FFNN) the information only moves in one direction, forward through layers. This means that in an FFNN, the inputs $x_i$ of the activation function $f$ are the outputs of the neurons in the preceding layer.
%
% The Universal Approximation Theorem tells us that no matter what our data set is, there is a Neural Network that can approximately approach the result and do the job. This result holds for any number of inputs and outputs \cite{95}.
%
% Thus the basic components of a neural network are stylized neurons consisting of a linear transformation that weights the importance of various inputs, followed by a non-linear activation function. A typical example for such an activation function is the logistic Sigmoid (\ref{eq:Sigmoid}).
%
% We assume that there are $L$ layers in our network with $l = 1,...,L$ indexing the layer and that different layers have different activation functions. Further we denote $w^l_{ij}$ as the weight for the connection from the $j$-th neuron in layer $l-1$ to the $i-th$ neuron in layer $l$. The bias of this neuron is written as $b^l_i$. The mathematical model for a FFNN is then reads:
% \begin{equation}
%     a_i^l = f^l(z_i^l) = f^l \left(\Sigma_{j=1}^{N_{l-1}}w_{ij}^l a_j^{l-1} + b_i^l\right)
% \label{eq:FFNN}
% \end{equation}
% where $l$ denotes the $l$-th layer and $N_l$ is the number of nodes in layer $l$. A FFNN that is fully-connected consisting of neurons that have non-linear activation functions, receives a weighted sum of the outputs of all neurons in the previous layer (figure \ref{fig:FFNN}). %--> Regnte med at det var denne du mente
%
% %\begin{figure}[H]
% %    \centering
% %    \includegraphics[width=\linewidth]{FFNN.png}
% %    \caption{In \textbf{A} we see that one data point from the input data  is connected (as visualized in \textbf{A}) to all neurons in the following layer. This process goes on until we reach the output layer.}
% %    \label{fig:FFNN}
% %\end{figure}
%
% We will be studying Franke's function which produces a certain type of terrain data. You can read more about Franke's function in our previous report \cite{97}.
%
% % Here: linear means that each node in the output layer has a linear activation function.
% % find picture to illustrate
%

% \subsubsection*{Structure of our neural network}
%
% The way we decided to structure our neural network is based on how to handle the design matrix containing the input data used for producing Franke's Function terrain data. We decided to take a slightly different approach to setting up the design matrix compared to in project 1 \cite{97}. Instead of assuming a polynomial of a given complexity as we did there, we now set up the design matrix as shown below in (\ref{eq:design}). This way the columns represent all the $x$ and $y$ values respectively, and each row represent each its own unique combination of the input points. The design matrix then contains all the inputs used for producing the terrain, so by the universal approximation theorem there should then exist a network which should produce Franke's Function terrain. The design matrix then takes the form:
%
% \begin{equation}
% X=
%     \begin{bmatrix}
%         x_1 & y_1\\
%         x_2 & y_1 \\
%         . & . \\
%         . & . \\
%         x_n & y_1 \\
%         x_1 & y_2 \\
%         x_2 & y_2 \\
%         . & . \\
%         . & . \\
%         x_n & y_2 \\
%         . & . \\
%         . & . \\
%         x_1 & y_n \\
%         x_2 & y_n \\
%         . & . \\
%         . & . \\
%         x_n & y_n \\
%     \end{bmatrix}
% \label{eq:design}
% \end{equation}
%
% Where $n$ is the number of points in one direction. We then define the number of input rows as $N=n\times n$. We can visualize the feed forward method like this:
%
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{Feed_forward.png}
%     \caption{The feed forward method.}
%     \label{fig:our_FFNN}
% \end{figure}
%
% Where $w_i$ is the input weights and $w_o$ is the output weights. The numbered sub-indexes up to $L$ denote the hidden layers. $N$ is the input data points and $n_h$ is the number of hidden neurons. The superscripts denote the dimensions of the matrices. Notice how we have changed the order of the input-weight matrix multiplication when calculating the $z$'s. The reason for changing the order has to with making the dimensions add up, because when following the standard feed-forward formulas the dimensions didn't add up.
%
% For our neural network we choose the quadratic cost function with a Ridge regularization parameter ($L_2$-norm), described as follows:
%
% \begin{equation}
%     \frac{1}{2}\sum_{i=1}^{n}(a_i - t_i)^2 + \lambda||w||_2^2.
% \end{equation}
%
% The reason for choosing the quadratic loss function, as well as the $L_2$-norm, is their versatility and simple derivatives. When we look at the terrain data from Franke's function it is somewhat unnecessary to us regression models and resampling methods with the Ridge regularisation parameter. The reason was discussed in our previous report \cite{97}, which was that the lack of heavy outliers.
%
% However, when we are analyzing the breast cancer data set, we might expect more heavy outliers, which could benefit from a regularization parameter.
%
% \subsection*{Activation functions}
%
% The use of activation functions is inspired by the action potential of a human brain neuron; depending on the incoming current, the neuron will either fire or not. Activation functions work in a similar way, the input to a single neuron is transformed by an activation function causing a non-linear relation between the input and output. It is in this way the network learns. There are many activation functions, but the ones we will experiment with and discuss in this project are mainly:
%
% \begin{itemize}
%     \item The Sigmoid function: $f(z)=\frac{1}{1+e^{-z}}$
%     \item Hyperbolic tangent: $f(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$
%     \item ReLU: $f(z)=\text{max}\{0,x\}$.
% \end{itemize}
%
% \subsubsection*{Sigmoid function}
%
% A big reason for using the Sigmoid is that it's range is between $[0, 1]$. This is naturally good when predicting probabilities. Another advantage when using the Sigmoid is that it is differentiable everywhere, where its analytical expression re-uses the sigmoid: $f'(x)=f(x)(1-f(x))$. This allows for effective computing in cases such as the back propogation algorithm which uses the derivative.
%
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=80mm]{Task b,c/sigmoid.png}
%     \caption{We see the sigmoid with its characteristic S-shape and horisontal asymptotes at $1$ and $0$. Note that Sigmoid$(0)=0.5$.}
%     \label{fig:Sigmoid}
% \end{figure}
%
% \subsubsection*{Hyperbolic tangent (Tanh)}
% The hyperbolic tangent is similar to the Sigmoid in its shape and characteristics, but Tanh's range is instead between $[-1, 1]$ and Tanh$(0)=0$. Similar to the Sigmoid, Tanh also reuses itself in its expression for its derivative: $f'(x)=1-f(x)^2$
%
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=80mm]{Task b,c/Tanh.png}
%     \caption{We see the hyperbolic tangent also with its characteristic S-shape, but with its horisontal asymptotes at $-1$ and $1$.}
%     \label{fig:Tanh}
% \end{figure}
%
% \subsubsection*{ReLU}
% ReLU is quite simple compared to the two mentioned above, in that it returns its input value if the input is above zero, and returns zero if the input is below zero. Its simplicity is one of the main reason for its wide usage in neural networks. For input values larger than zero it's also effective in avoiding vanishing gradients which the two others mentioned above are susceptible to (This will be discussed further in the next section). Though it must be noted that also the ReLU could be sucseptible to vanishing gradients given that the derivative for negative input values are zero. One minor problem when using ReLU is that it's not differentiable at $x=0$. This isn't necessarily a big problem as you can just define the derivative there to be zero, which is correct when $x$ approaches zero from the left. But it is something to be wary of when using the ReLU.
%
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=80mm]{Task b,c/ReLU.png}
%     \caption{We see the ReLU with its non-differentiable sharp turn at $x=0$.}
%     \label{fig:ReLU}
% \end{figure}
%
% \subsection*{Initialization of the weights and biases}
%
% The initialization of the weights and biases is seemingly arbitrary, since the network in theory should change them accordingly to fit the target data. But there are naturally some initializations that will minimize the number of epochs necessary to reach convergence, and there are some that in practice won't reach convergence at all. The optimal choices of initialization will also depend on the chosen activation function in the network. Lets use the Sigmoid as an activation function as an example. If you choose very large, or very small weights, the gradient will become very small (figure \ref{fig:sigmoid}), causing the network to learn at an extremely slow rate. This is known as a vanishing gradient.
%
% However, by choosing a different activation function, say the ReLU or Tanh($x$), we might run into different problems. If we again initialize the weights to be very large, we can see that the derivative of the activation grows proportionally large (for certain areas) as the weights increase (figure \ref{fig:Exploding_gradients}), which in turn leads to what's normally called "exploding gradients". This problem will then only become worse as the number of layers are increased, though mainly for ReLU, since the proportionality factor from the weights carry through for each layer.
%
% The neural network can actually also experience exploding gradients when using the Sigmoid function. But if we were to use biases equal to zero as in the example above it wouldn't happen. This is because the Sigmoid is bounded between $0$ and $1$, with $\text{Sig}(0)=\frac{1}{2}$ and $\text{Sig}(1)\approx 0.73$. Thus, $\text{Sigmoid}(\text{Sigmoid}(x))$ is bounded between $0.5$ and $0.73$. As the number of layers increase, the resulting slope will then only become flatter and flatter. However, if a bias of value $b=\frac{w}{2}$ is added to the input in each layer, the gradient will start exploding, as shown in figure \ref{fig:exploding_sigmoid}. Though, it has to be noted that the chances of achieving such an exploding gradient when using the Sigmoid seems rather unlikely, as long as the weights and biases are not initialized in this specific way.
%
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]
%     {Task b,c/sig_der_example.png}
%     \caption{The Sigmoid function and its derivative. We see that for large $x$-values the derivative converges towards zero.}
%     \label{fig:sigmoid}
% \end{figure}
%
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{Task b,c/ReLU_der_example.png}
%     \caption{In the figure above we can see that the derivatives grow proportionally large as the weights increase. The difference between the two functions can be seen as the derivative of Tanh only has one spike around $x=0$, while the derivative of the ReLU function is unbounded as $x\xrightarrow{}\infty$}.
%     \label{fig:Exploding_gradients}
% \end{figure}
%
% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{Task b,c/sig_sig_der_example.png}
%     \caption{In the figure above we can see that when the input of the second layer is specified as $z=\text{Sig}(w\cdot x - w/2)$ the derivative starts to grow. This would continue to happen for each layer ultimately causing an exploding gradient. This is in contrast to using $b=0$ which would cause a vanishing gradient.}
%     \label{fig:exploding_sigmoid}
% \end{figure}
%


\end{document}